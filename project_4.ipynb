{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.linalg import pinv, pinvh\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import umap\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from scipy.sparse.csgraph import floyd_warshall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mutual_knn_graph(X, k=5):\n",
    "    \"\"\"\n",
    "    Construct a mutual k-NN graph and weight edges using a Gaussian kernel.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Data matrix where rows are samples.\n",
    "        k (int): Number of neighbors for k-NN.\n",
    "        sigma (float): Gaussian kernel parameter.\n",
    "\n",
    "    Returns:\n",
    "        adj_matrix (np.ndarray): Symmetric weighted adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute k-NN graph (sparse adjacency matrix)\n",
    "    knn_graph = kneighbors_graph(X, n_neighbors=k, mode='distance', include_self=False)\n",
    "    knn_distances = knn_graph.toarray()\n",
    "\n",
    "    # Symmetrize to get mutual k-NN graph\n",
    "    mutual_knn = np.maximum(knn_distances, knn_distances.T)\n",
    "\n",
    "    # return adj_matrix\n",
    "    return mutual_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_graph_connected(adj_matrix):\n",
    "    \"\"\"\n",
    "    Check if the graph is fully connected.\n",
    "\n",
    "    Parameters:\n",
    "        adj_matrix (np.ndarray): Weighted adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "        connected (bool): True if the graph is fully connected, False otherwise.\n",
    "    \"\"\"\n",
    "    # Use scipy's connected_components to check connectivity\n",
    "    n_components, _ = connected_components(csgraph=adj_matrix, directed=False)\n",
    "    return n_components == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_commute_time_distance(knn_graph):\n",
    "#     \"\"\"\n",
    "#     Compute the Commute Time Distance for a given k-NN graph.\n",
    "\n",
    "#     Parameters:\n",
    "#         knn_graph (np.ndarray): Adjacency matrix of the k-NN graph (NxN).\n",
    "#                                 The graph should be symmetric and connected.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Commute time distance matrix (NxN).\n",
    "#     \"\"\"\n",
    "#     # Ensure the graph is symmetric\n",
    "#     if not np.allclose(knn_graph, knn_graph.T):\n",
    "#         raise ValueError(\"Input graph must be symmetric.\")\n",
    "\n",
    "#     # Compute the degree matrix\n",
    "#     degrees = np.sum(knn_graph, axis=1)\n",
    "#     degree_matrix = np.diag(degrees)\n",
    "#     print('Degree Matrix Computed')\n",
    "\n",
    "#     # Compute the Laplacian matrix\n",
    "#     laplacian = degree_matrix - knn_graph\n",
    "#     print('Laplacian Matrix Computed')\n",
    "    \n",
    "\n",
    "#     # Compute the pseudoinverse of the Laplacian\n",
    "#     laplacian_pinv = pinv(laplacian)\n",
    "#     print('Laplacian Pseudoinverse Computed')\n",
    "\n",
    "#     n = knn_graph.shape[0]\n",
    "#     ctd_matrix = np.zeros((n, n))\n",
    "\n",
    "#     # Compute the commute time distance matrix\n",
    "#     for i in range(n):\n",
    "#         for j in range(n):\n",
    "#             ctd_matrix[i, j] = laplacian_pinv[i, i] + laplacian_pinv[j, j] - 2 * laplacian_pinv[i, j]\n",
    "\n",
    "#     # ensure vtd_matrix is symmetric\n",
    "#     ctd_matrix = np.maximum(ctd_matrix, ctd_matrix.T)\n",
    "    \n",
    "#     return ctd_matrix.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_commute_time_distance(knn_graph):\n",
    "    \"\"\"\n",
    "    Compute the Commute Time Distance for a given k-NN graph.\n",
    "\n",
    "    Parameters:\n",
    "        knn_graph (np.ndarray): Adjacency matrix of the k-NN graph (NxN).\n",
    "                                The graph should be symmetric and connected.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Commute time distance matrix (NxN).\n",
    "    \"\"\"\n",
    "    # Ensure the graph is symmetric\n",
    "    if not np.allclose(knn_graph, knn_graph.T):\n",
    "        raise ValueError(\"Input graph must be symmetric.\")\n",
    "    \n",
    "    # Compute the degree matrix\n",
    "    degrees = np.sum(knn_graph, axis=1)\n",
    "    degree_matrix = np.diag(degrees)\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    laplacian = degree_matrix - knn_graph\n",
    "\n",
    "    # Compute the pseudoinverse of the Laplacian\n",
    "    laplacian_pinv = pinvh(laplacian)  # Use pinvh for symmetric matrices\n",
    "\n",
    "    # Vectorized computation of commute time distance\n",
    "    diag_elements = np.diag(laplacian_pinv)\n",
    "    ctd_matrix = diag_elements[:, None] + diag_elements[None, :] - 2 * laplacian_pinv\n",
    "\n",
    "    # Return the CTD matrix\n",
    "    return ctd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shortest_path_distance(knn_graph):\n",
    "    \"\"\"\n",
    "    Compute the Shortest Path Distance for a given k-NN graph.\n",
    "\n",
    "    Parameters:\n",
    "        knn_graph (np.ndarray): Adjacency matrix of the k-NN graph (NxN).\n",
    "                                The graph should be symmetric and connected.\n",
    "                                Non-edges should have a weight of 0.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Shortest path distance matrix (NxN).\n",
    "    \"\"\"\n",
    "    # Ensure the graph is symmetric\n",
    "    if not np.allclose(knn_graph, knn_graph.T):\n",
    "        raise ValueError(\"Input graph must be symmetric.\")\n",
    "\n",
    "    # Replace zeros (non-edges) with infinity for Floyd-Warshall algorithm\n",
    "    graph = np.where(knn_graph > 0, knn_graph, np.inf)\n",
    "    np.fill_diagonal(graph, 0)  # Set diagonal to 0 (distance to self is zero)\n",
    "\n",
    "    # Compute shortest path distances\n",
    "    shortest_path_distances = floyd_warshall(csgraph=graph, directed=False)\n",
    "\n",
    "    return shortest_path_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_data_3D(reduced_data, test_labels):\n",
    "    \"\"\"\n",
    "    Plots reduced data points in 3D, colored by their labels.\n",
    "\n",
    "    Parameters:\n",
    "        reduced_data (np.ndarray): Reduced data with 3 dimensions.\n",
    "        test_labels (np.ndarray): Labels corresponding to data points.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(test_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))  # Use a colormap with enough colors\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = test_labels == label\n",
    "        ax.scatter(\n",
    "            reduced_data[idx, 0], reduced_data[idx, 1], reduced_data[idx, 2],\n",
    "            color=colors(i), label=f\"Label {label}\", s=5, alpha=0.8\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"3D Plot of Reduced Data\")\n",
    "    ax.set_xlabel(\"Dimension 1\")\n",
    "    ax.set_ylabel(\"Dimension 2\")\n",
    "    ax.set_zlabel(\"Dimension 3\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_data_2D(reduced_data, test_labels):\n",
    "    \"\"\"\n",
    "    Plots reduced data points in 2D, colored by their labels.\n",
    "\n",
    "    Parameters:\n",
    "        reduced_data (np.ndarray): Reduced data with 2 dimensions.\n",
    "        test_labels (np.ndarray): Labels corresponding to data points.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(test_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))  # Use a colormap with enough colors\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = test_labels == label\n",
    "        plt.scatter(\n",
    "            reduced_data[idx, 0], reduced_data[idx, 1],\n",
    "            color=colors(i), label=f\"Label {label}\", s=5, alpha=0.8\n",
    "        )\n",
    "\n",
    "    plt.title(\"2D Plot of Reduced Data\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "def plot_reduced_data_3D_gif(reduced_data, test_labels, output_path=\"3D_plot_classes.gif\"):\n",
    "    \"\"\"\n",
    "    Creates a 3D plot of reduced data points, adding one class at a time,\n",
    "    and saves the result as a GIF.\n",
    "\n",
    "    Parameters:\n",
    "        reduced_data (np.ndarray): Reduced data with 3 dimensions.\n",
    "        test_labels (np.ndarray): Labels corresponding to data points.\n",
    "        output_path (str): Path to save the generated GIF.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(test_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))  # Use a colormap with enough colors\n",
    "\n",
    "    frames_dir = \"temp_frames\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Configure plot\n",
    "    ax.set_title(\"3D Plot of Reduced Data\")\n",
    "    ax.set_xlabel(\"Dimension 1\")\n",
    "    ax.set_ylabel(\"Dimension 2\")\n",
    "    ax.set_zlabel(\"Dimension 3\")\n",
    "\n",
    "    frame_paths = []\n",
    "\n",
    "    # Add classes one by one\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = test_labels == label\n",
    "        ax.scatter(\n",
    "            reduced_data[idx, 0], reduced_data[idx, 1], reduced_data[idx, 2],\n",
    "            color=colors(i), label=f\"Label {label}\", s=5, alpha=0.8\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "        # Save the current frame\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{i}.png\")\n",
    "        plt.savefig(frame_path)\n",
    "        frame_paths.append(frame_path)\n",
    "\n",
    "    # Create GIF\n",
    "    images = [imageio.imread(frame_path) for frame_path in frame_paths]\n",
    "    imageio.mimsave(output_path, images, fps=1)\n",
    "\n",
    "    # Cleanup temporary frames\n",
    "    for frame_path in frame_paths:\n",
    "        os.remove(frame_path)\n",
    "    os.rmdir(frames_dir)\n",
    "\n",
    "    print(f\"GIF saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "def plot_reduced_data_2D_gif(reduced_data, test_labels, output_path=\"2D_plot_classes.gif\"):\n",
    "    \"\"\"\n",
    "    Creates a 2D plot of reduced data points, adding one class at a time,\n",
    "    and saves the result as a GIF.\n",
    "\n",
    "    Parameters:\n",
    "        reduced_data (np.ndarray): Reduced data with 2 dimensions.\n",
    "        test_labels (np.ndarray): Labels corresponding to data points.\n",
    "        output_path (str): Path to save the generated GIF.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(test_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))  # Use a colormap with enough colors\n",
    "\n",
    "    frames_dir = \"temp_frames\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    frame_paths = []\n",
    "\n",
    "    # Create 2D plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = test_labels == label\n",
    "        plt.scatter(\n",
    "            reduced_data[idx, 0], reduced_data[idx, 1],\n",
    "            color=colors(i), label=f\"Label {label}\", s=5, alpha=0.8\n",
    "        )\n",
    "        plt.title(\"2D Plot of Reduced Data\")\n",
    "        plt.xlabel(\"Dimension 1\")\n",
    "        plt.ylabel(\"Dimension 2\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the current frame\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{i}.png\")\n",
    "        plt.savefig(frame_path)\n",
    "        frame_paths.append(frame_path)\n",
    "\n",
    "    # Create GIF\n",
    "    images = [imageio.imread(frame_path) for frame_path in frame_paths]\n",
    "    imageio.mimsave(output_path, images, fps=1)\n",
    "\n",
    "    # Cleanup temporary frames\n",
    "    for frame_path in frame_paths:\n",
    "        os.remove(frame_path)\n",
    "    os.rmdir(frames_dir)\n",
    "\n",
    "    print(f\"GIF saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent(model, data_loader, device):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for _, (data, label) in enumerate(data_loader):\n",
    "          mu, _ = model.encoder(data.to(device))\n",
    "          latents.append(mu.cpu())\n",
    "          labels.append(label.cpu())\n",
    "\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    \n",
    "    return latents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_reduction(latents, n_neighbors=15, min_dist=0.1, n_components=2):\n",
    "    # Initialize UMAP model\n",
    "    # umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components)\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components)\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_reduction_3D(latents, n_neighbors=15, min_dist=0.1, n_components=3):\n",
    "    # Initialize UMAP model\n",
    "    # umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components)\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components)\n",
    "    embedding = reducer.fit_transform(latents)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renke\\AppData\\Local\\Temp/ipykernel_23180/10316377.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 8\n",
    "\n",
    "model_name = \"vae_8new_layout.pth\"\n",
    "\n",
    "vae = VAE(z_dim=z_dim)\n",
    "vae.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "testset = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_variables, lables = get_latent(vae, test_loader,\n",
    "                       device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 1000 images\n",
    "n_samples = 1000\n",
    "sampled_indices = np.random.choice(latent_variables.shape[0], n_samples, replace=False)\n",
    "\n",
    "latent_variables = latent_variables[sampled_indices]\n",
    "lables = lables[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_variables.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions to 2D using UMAP\n",
    "latents_2d = umap_reduction(latent_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_3d = umap_reduction_3D(latent_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_3D(latents_3d, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_2D(latents_2d, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to 2D_classes.gif\n"
     ]
    }
   ],
   "source": [
    "plot_reduced_data_2D_gif(latents_2d, lables, output_path=\"2D_classes.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_latent = construct_mutual_knn_graph(latent_variables, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is connected\n"
     ]
    }
   ],
   "source": [
    "if is_graph_connected(knn_latent):\n",
    "    print('Graph is connected')\n",
    "else:\n",
    "    print('Graph is not connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute commute time distance\n",
    "ctd_latent = compute_commute_time_distance(knn_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute shortest path distance\n",
    "spd_latent = compute_shortest_path_distance(knn_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store spd_latent and ctd_latent in text files\n",
    "# np.save('spd_latent_reduced_data.npy', spd_latent)\n",
    "# np.save('ctd_latent_reduced_data.npy', ctd_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ctd_latent\n",
    "# ctd_latent = np.load('ctd_latent.npy')\n",
    "# spd_latent = np.load('spd_latent.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MDS on the commute time distance matrix\n",
    "mds_3D = MDS(n_components=3, dissimilarity='precomputed')\n",
    "reduced_latent_ctd_3D = mds_3D.fit_transform(ctd_latent)\n",
    "\n",
    "# store reduced_latent_3D in a text file\n",
    "# np.save('reduced_latent_ctd_3D.npy', reduced_latent_ctd_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_latent_ctd_3D = np.load(\"reduced_latent_ctd_3D.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_3D(reduced_latent_ctd_3D, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to 3D_classes.gif\n"
     ]
    }
   ],
   "source": [
    "plot_reduced_data_3D_gif(reduced_latent_ctd_3D, lables, output_path=\"3D_classes.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Perform MDS on the commute time distance matrix\n",
    "mds_2D = MDS(n_components=2, dissimilarity='precomputed')\n",
    "reduced_latent_ctd_2D = mds_2D.fit_transform(ctd_latent)\n",
    "\n",
    "# store reduced_latent_2D in a text file\n",
    "# np.save('reduced_latent_ctd_2D.npy', reduced_latent_ctd_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reduced_latent_ctd_2D.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8736/2539408866.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreduced_latent_ctd_2D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reduced_latent_ctd_2D.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\renke\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reduced_latent_ctd_2D.npy'"
     ]
    }
   ],
   "source": [
    "reduced_latent_ctd_2D = np.load('reduced_latent_ctd_2D.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_2D(reduced_latent_ctd_2D, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to 2D_classes.gif\n"
     ]
    }
   ],
   "source": [
    "plot_reduced_data_2D_gif(reduced_latent_ctd_2D, lables, output_path=\"2D_classes.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_3D = MDS(n_components=3, dissimilarity='precomputed')\n",
    "reduced_latent_spd_3D = mds_3D.fit_transform(spd_latent)\n",
    "\n",
    "# store reduced_latent_3D in a text file\n",
    "np.save('reduced_latent_spd_3D.npy', reduced_latent_spd_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_latent_spd_3D = np.load('reduced_latent_spd_3D.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_3D(reduced_latent_spd_3D, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_2D = MDS(n_components=2, dissimilarity='precomputed')\n",
    "reduced_latent_spd_2D = mds_2D.fit_transform(spd_latent)\n",
    "\n",
    "# store reduced_latent_2D in a text file\n",
    "np.save('reduced_latent_spd_2D.npy', reduced_latent_spd_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_latent_spd_2D = np.load(\"reduced_latent_spd_2D.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_data_2D(reduced_latent_spd_2D, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_plot_from_distance_matrix(distance_matrix, labels, save_path):\n",
    "    \"\"\"\n",
    "    Function to create a Silhouette Plot using a precomputed distance matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - distance_matrix: Precomputed pairwise distance matrix.\n",
    "    - labels: Cluster labels for each point (from clustering).\n",
    "    - save_path: File path to save the silhouette plot.\n",
    "    \"\"\"\n",
    "    # Ensure distance matrix is square\n",
    "    assert distance_matrix.shape[0] == distance_matrix.shape[1], \"Distance matrix must be square\"\n",
    "\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    silhouette_vals = silhouette_samples(distance_matrix, labels, metric=\"precomputed\")\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Variables to keep track of the silhouette value ranges\n",
    "    y_lower = 10  # Starting point for the first silhouette bar\n",
    "\n",
    "    # Define a list of tab colors (enough for up to 10 clusters)\n",
    "    tab_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:gray', 'tab:olive', 'tab:cyan', 'tab:pink', 'tab:purple', 'tab:brown']\n",
    "\n",
    "    # Plot silhouette values for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate silhouette scores for samples in the current cluster\n",
    "        ith_cluster_silhouette_values = silhouette_vals[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        # Number of points in the current cluster\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        # Use tab colors for the clusters\n",
    "        color = tab_colors[i % len(tab_colors)]\n",
    "        \n",
    "        # Fill plot for each cluster\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, ith_cluster_silhouette_values,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Calculate and annotate average silhouette score for the cluster\n",
    "        avg_silhouette_score = np.mean(ith_cluster_silhouette_values)\n",
    "        ax.text(0.05, y_lower + 0.5 * size_cluster_i, f'{avg_silhouette_score:.2f}', \n",
    "                color='black', fontsize=10, weight='bold')\n",
    "\n",
    "        # Label cluster\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Move the y_lower boundary for the next cluster\n",
    "        y_lower = y_upper + 10  # 10 for spacing between clusters\n",
    "\n",
    "    # Configure the plot\n",
    "    ax.set_title(\"Silhouette Plot for the Clusters (Precomputed Distance Matrix)\")\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "    ax.set_ylabel(\"Cluster Label\")\n",
    "\n",
    "    # Draw a vertical line for average silhouette score across all points\n",
    "    ax.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\", label='Average Silhouette Score')\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_yticks([])  # Clear the y-axis labels/ticks\n",
    "    ax.set_xlim([-1, 1])  # Silhouette values range between -1 and 1\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot_from_distance_matrix(ctd_latent, lables, 'silhouette_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_plot(data, labels, save_path):\n",
    "    \"\"\"\n",
    "    Function to create a Silhouette Plot with fixed tab colors and average silhouette scores.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The dataset (after PCA transformation).\n",
    "    - labels: Cluster labels for each point (from GMM clustering).\n",
    "    \"\"\"\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    silhouette_vals = silhouette_samples(data, labels)\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Variables to keep track of the silhouette value ranges\n",
    "    y_lower = 10  # Starting point for the first silhouette bar\n",
    "\n",
    "    # Define a list of tab colors (enough for up to 10 clusters)\n",
    "    tab_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:gray', 'tab:olive', 'tab:cyan', 'tab:pink', 'tab:purple', 'tab:brown']\n",
    "\n",
    "    # Plot silhouette values for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate silhouette scores for samples in the current cluster\n",
    "        ith_cluster_silhouette_values = silhouette_vals[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        # Number of points in the current cluster\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        # Use tab colors for the clusters\n",
    "        color = tab_colors[i % len(tab_colors)]\n",
    "        \n",
    "        # Fill plot for each cluster\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, ith_cluster_silhouette_values,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Calculate and annotate average silhouette score for the cluster\n",
    "        avg_silhouette_score = np.mean(ith_cluster_silhouette_values)\n",
    "        ax.text(0.05, y_lower + 0.5 * size_cluster_i, f'{avg_silhouette_score:.2f}', \n",
    "                color='black', fontsize=10, weight='bold')\n",
    "\n",
    "        # Label cluster\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Move the y_lower boundary for the next cluster\n",
    "        y_lower = y_upper + 10  # 10 for spacing between clusters\n",
    "\n",
    "    # Configure the plot\n",
    "    ax.set_title(\"Silhouette Plot for the Clusters\")\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "    ax.set_ylabel(\"Cluster Label\")\n",
    "\n",
    "    # Draw a vertical line for average silhouette score across all points\n",
    "    ax.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\", label='Average Silhouette Score')\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_yticks([])  # Clear the y-axis labels/ticks\n",
    "    ax.set_xlim([-1, 1])  # Silhouette values range between -1 and 1\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(latent_variables, lables, 'silhouette_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
